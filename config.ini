# This file is to store all the configurations used so far



[PPO_Baseline]
# Environment Config
ENV_N = 32
CHANNELS_N = 3
ACT_N = 4
OBSTACLE_PCT = 0.1
RESOURCE_PCT = 0.15
WASTE_MOVE_PENALTY = 0.01
DEATH_PENALTY = 1
FINISH_REWARD = 2
TRAIN_EPISODES = 1
SIM_EPISODES = 1
SIM_FPS = 5

# Agent Config
POLICY = PPO
LR = 1e-4
GAMMA = 0.99
EPS_CLIP = 0.2
EPOCH_K = 30
AGENT_HP = 2
AGENT_RUN_DELTA = 0.01

[DDQN_Baseline]
# Environment Config
ENV_N = 32
CHANNELS_N = 3
ACT_N = 4
OBSTACLE_PCT = 0.1
RESOURCE_PCT = 0.2
WASTE_MOVE_PENALTY = 0.01
DEATH_PENALTY = 1
FINISH_REWARD = 2
TRAIN_EPISODES = 1
SIM_EPISODES = 1
SIM_FPS = 5

# Agent Constant
POLICY = DDQN
LR = 1e-4
GAMMA = 0.95
EPS = 0.99
EPS_MIN = 0.1
EPS_DECAY = 0.9999
BUFFER_SIZE = 1000000
BATCH_SIZE = 32
AGENT_HP = 2
AGENT_RUN_DELTA = 0.01
AGENT_LEARN_COUNTER = 100

[PPO_Baseline-v2]
# Environment Config
ENV_N = 32
CHANNELS_N = 3
ACT_N = 4
OBSTACLE_PCT = 0.1
RESOURCE_PCT = 0.2
WASTE_MOVE_PENALTY = 0.01
DEATH_PENALTY = 1
FINISH_REWARD = 2
TRAIN_EPISODES = 100
SIM_EPISODES = 1
SIM_FPS = 5

# Agent Config
POLICY = PPO
LR = 1e-4
GAMMA = 0.95
EPS_CLIP = 0.2
EPOCH_K = 30
AGENT_HP = 2
AGENT_RUN_DELTA = 0.01